---
title: "Exam 1"
author: "Kaylyn Zipp"
date: "2/17/2021"
output: html_document
---

1. Import this dataset into R and inspect the first several rows of your data

Looking over the data briefly
```{r}
Data<-read.csv('Exam 1 Data.csv')

head(Data)

summary(Data)

```


2. Fit a linear model that assumes your response is a function of x1, x2, and x3. Include an interaction
between x1 and x2 only (i.e., do not include an interaction between your categorical variables and any
other variables).

Creating the model 
```{r}
fit <- lm(y ~ x1*x2 + x3, data = Data)
summary(fit)
```

y= b0 + b1 * x1 + b2 * x2 + b3 * x3b + b4 * x3c + b5 x1 * x2 


3. Interpret the effect of variable x1 when x2 = -1


y= b0 + b1 * x1 + b2 * x2 + b3 * x3b + b4 * x3c + b5 x1 * x2 

Collect the terms associated with x1

b1 * x1 + b5 * x1 * x2

Now lets isolate

x1(b1+b5*x2)

```{r}
b<-coef(fit)
b1<-b[2]
b5<-b[6]

effect_of_variable_x1_neg1<-b1+b5*-1
print(effect_of_variable_x1_neg1)
```


4. Interpret the effect of variable x1 when x2 = 1

Using the isolation from above:
x1(b1+b5*x2)

```{r}
b<-coef(fit)
b1<-b[2]
b5<-b[6]

effect_of_variable_x1_1<-b1+b5*1
print(effect_of_variable_x1_1)
```


5. Interpret the effect of variable x3

Examine the coefficients


```{r}
b<-coef(fit)
#y= b0 + b1 * x1 + b2 * x2 + b3 * x3b + b4 * x3c + b5 * x1 * x2 
```

The difference in y (when all other variables held constant) between category b and a is 0.007804
The difference in y (when all other variables held constant) between category c and a is -0.210479

6. Describe how R codes the categorical variable x3. Demonstrate by reporting the first 5 values of
variables derived from x3

To code a categorical variable R creates dummy variables. R creates k-1 dummy variables where k is the number of levels of the categorical variable. For example if there are 4 levels corresponding to habitat type, it will create 3 dummy variables. Here R is creating 2 dummy variables. A is the reference so there is no dummy variable associated with it. There is a dummy variable associated with variable b that equals 1 if a factor level is b and equals 0 otherwise. For variable c, it equals 1 if the factor level is c otherwise it is 0. 


```{r}
cbind(Data$x3[1:5],
ifelse(Data$x3 == 'b', 1, 0)[1:5],
ifelse(Data$x3 == 'c', 1, 0)[1:5])
```


7. Derive the test statistic and p-value associated with the interaction between x1 and x2. What is the
null hypothesis assumed by the "lm()" function? Do we reject or fail to reject this null hypothesis?
Defend your answer.


```{r}
#t-statistic
b
ts <- coef(fit)[6] / summary(fit)[['coefficients']][6, 2]
print(ts)
summary(fit)[['coefficients']][6, 3]

```

```{r}
# p-value
nrow(Data)
#100
length(coef(fit))
#6

pt(ts, df = 93)*2

pvalue <- pt(q=-abs(ts), df = 94) + (1 - pt(q=abs(ts), df = 94))
print(pvalue)
```

Checking work

```{r}
summary(fit)[['coefficients']][6, 4]
```

The null hypothesis is that the slope coefficient associated with interaction of x1 and x2 = 0. We fail to reject the null hypothesisbecause the p-value is larger than any reasonable α.

8. assume you have the following realizations of random variable Y :
y = (3, 8, 7)
Further assume realizations of the random variable Y are Gaussian distributed:
y ∼ Gaussian(µ, σ2).Fix σ 2 = 1 and µ = 8, and evaluate the probability density at each of your 3 realizations.

Start with at 3; since it is further from the mean it has lower probability 
```{r}
mu <- 8 # theoretical expected value
v <- 1 # theoretical variance
y <- 3 # realization of a random variable
# plug these values into the PDF
Realizedat3<-1 / (sqrt(2 * pi * v)) * exp(- (y - mu) ^ 2 / (2 * v))

print(Realizedat3)
```


```{r}
mu <- 8 # theoretical expected value
v <- 2 # theoretical variance
y <- 8 # realization of a random variable
# plug these values into the PDF
Realizedat8<-1 / (sqrt(2 * pi * v)) * exp(- (y - mu) ^ 2 / (2 * v))

print(Realizedat8)
```

```{r}
mu <- 8 # theoretical expected value
v <- 2 # theoretical variance
y <- 7 # realization of a random variable
# plug these values into the PDF
Realizedat7<-1 / (sqrt(2 * pi * v)) * exp(- (y - mu) ^ 2 / (2 * v))

print(Realizedat7)
```

Since 8 and 7 are closer to the mean they have hihger probabilities. 


9. What is a type I error? What is a p-value? How are the two quantities related?

A type 1 error is when you fasely reject the null hypothesis that is true. 

A p-value is the probability of observing a more extreme value of a test statistic, under the assumptions of the null 
The p-value of our test statistic determines if we reject or fail to reject the null hypothesis. We only reject the null hypothesis if our p-value is small to avoid falsely rejecting the null hypothesis, which would be commiting a type 1 error. 

10. What is a fundamental assumption we must make to derive inference about regression coefficients of a
linear model?

We assume that the sample means are coming from independent samples and that the distribution of those idependent sample means is normal, Gaussian. We must also assume that the relationship is best described by a linear model.   




